{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RNN in ``tf`` \n",
    "\n",
    "LSTM model in ``tf``:\n",
    "\n",
    "<center>\n",
    "<img src=\"images/tf-lstm.png\" alt=\"\" style=\"width: 600px;\"/> \n",
    "Source: https://cloud.google.com/blog/big-data/2017/01/learn-tensorflow-and-deep-learning-without-a-phd\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GRU model in ``tf``:\n",
    "\n",
    "<center>\n",
    "<img src=\"images/tf-gru.png\" alt=\"\" style=\"width: 600px;\"/> \n",
    "Source: https://cloud.google.com/blog/big-data/2017/01/learn-tensorflow-and-deep-learning-without-a-phd\n",
    "<center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning to add in ``tf``\n",
    "\n",
    "Source: http://projects.rajivshah.com/blog/2016/04/05/rnn_addition/ \n",
    "\n",
    "The objective of this code developed by Rajiv Shah is to train a RNN for adding a sequence of integers. In this acse we must process all the sequence in order to produce a result. We will use the ``tf`` implementation of the ``seq2seq`` architecure, originally proposed by Sutskever, Vinyals and Le in 2014. \n",
    "\n",
    "The architecture diagram from their paper is:\n",
    "\n",
    "<center>\n",
    "<img src=\"images/seq2seq.png\" alt=\"\" style=\"width: 800px;\"/> \n",
    "Source: https://arxiv.org/abs/1409.3215\n",
    "<center>\n",
    "\n",
    "Rectangles are recurrent layers. Encoder receives ``[A, B, C]`` sequence as inputs. We don't care about encoder outputs, only about the hidden state it accumulates while reading the sequence. \n",
    "\n",
    "After input sequence ends, encoder passes its final state to decoder, which receives ``[<EOS>, W, X, Y, Z]`` and is trained to output ``[W, X, Y, Z, <EOS>]``. \n",
    "\n",
    "``tf`` implementation of the ``seq2seq`` allows sequences of different lenghts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Import basic libraries\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.ops import rnn_cell\n",
    "from tensorflow.python.ops import rnn\n",
    "from tensorflow.python.ops import seq2seq\n",
    "from numpy import sum\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import *\n",
    "%matplotlib inline  \n",
    "tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will define first a set of hyperparameters, being the most important ``num_units``, that is the parameter that represents the internal memory in the basic LSTM cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_units = 50   # units in LSTM cell\n",
    "input_size = 1   # input dimension\n",
    "batch_size = 50    \n",
    "seq_len = 7      # sequence lenght\n",
    "drop_out = 0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we can write an auxiliar function to generate random sequences of integers (and the result of their addition):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Creates our random sequences\n",
    "def gen_data(min_length=5, max_length=15, n_batch=50):\n",
    "    \n",
    "    X = np.concatenate([np.random.randint(10,size=(n_batch, max_length, 1))],\n",
    "                       axis=-1)\n",
    "    y = np.zeros((n_batch,))\n",
    "    # Compute masks and correct values\n",
    "    for n in range(n_batch):\n",
    "        # Randomly choose the sequence length\n",
    "        length = np.random.randint(min_length, max_length)\n",
    "        X[n, length:, 0] = 0\n",
    "        # Sum the dimensions of X to get the target value\n",
    "        y[n] = np.sum(X[n, :, 0]*1)\n",
    "    return (X,y)\n",
    "\n",
    "print gen_data(2,seq_len,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are ready to start the model construction phase:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Model architecture\n",
    "\n",
    "num_layers = 2\n",
    "cell = rnn_cell.BasicLSTMCell(num_units)\n",
    "cell = rnn_cell.MultiRNNCell([cell] * num_layers)\n",
    "cell = rnn_cell.DropoutWrapper(cell,output_keep_prob=drop_out)\n",
    "\n",
    "# Create placeholders for X and y\n",
    "\n",
    "inputs = [tf.placeholder(tf.float32,shape=[batch_size,1]) \n",
    "          for _ in range(seq_len)]\n",
    "result = tf.placeholder(tf.float32, shape=[batch_size])\n",
    "\n",
    "# We initialize the initial cell state to 0\n",
    "initial_state = cell.zero_state(batch_size, tf.float32)\n",
    "\n",
    "# We use a rnn_decoder model\n",
    "# outputs: A list of the same length as decoder_inputs \n",
    "# of 2D Tensors with shape [batch_size x input_size] containing \n",
    "# generated outputs\n",
    "outputs, states = seq2seq.rnn_decoder(inputs, \n",
    "                                      initial_state, \n",
    "                                      cell, \n",
    "                                      scope ='rnnln')\n",
    "\n",
    "# We are only interested in the final decoder output value\n",
    "outputs2 = outputs[-1]\n",
    "\n",
    "# Tranformation of the final LSTM output value to a real value\n",
    "\n",
    "W_o = tf.Variable(tf.random_normal([num_units,input_size], stddev=0.01))     \n",
    "b_o = tf.Variable(tf.random_normal([input_size], stddev=0.01))\n",
    "outputs3 = tf.matmul(outputs2, W_o) + b_o\n",
    "\n",
    "# Definition of the mean square loss function\n",
    "\n",
    "cost = tf.pow(tf.sub(tf.reshape(outputs3, [-1]), result),2)\n",
    "train_op = tf.train.RMSPropOptimizer(0.005, 0.2).minimize(cost) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### Generate Validation Data\n",
    "\n",
    "tempX,y_val = gen_data(6,seq_len,batch_size)\n",
    "X_val = []\n",
    "for i in range(seq_len):\n",
    "    X_val.append(tempX[:,i,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "##Session\n",
    "\n",
    "sess = tf.Session()\n",
    "sess.run(tf.initialize_all_variables())\n",
    "train_score =[]\n",
    "val_score= []\n",
    "x_axis=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "num_epochs=10000\n",
    " \n",
    "for k in tqdm(range(1,num_epochs)):\n",
    "\n",
    "    #Generate Data for each epoch\n",
    "    tempX,y = gen_data(5,seq_len,batch_size)\n",
    "    X = []\n",
    "    for i in range(seq_len):\n",
    "        X.append(tempX[:,i,:])\n",
    "\n",
    "    #Create the dictionary of inputs to feed into sess.run\n",
    "    temp_dict = {inputs[i]:X[i] for i in range(seq_len)}\n",
    "    temp_dict.update({result: y})\n",
    "\n",
    "    _,c_train = sess.run([train_op,cost],feed_dict=temp_dict)   \n",
    "    #perform an update on the parameters\n",
    "\n",
    "    val_dict = {inputs[i]:X_val[i] for i in range(seq_len)}  \n",
    "    #create validation dictionary\n",
    "    \n",
    "    val_dict.update({result: y_val})\n",
    "    c_val = sess.run([cost],feed_dict = val_dict )            \n",
    "    #compute the cost on the validation set\n",
    "    \n",
    "    if (k%100==0):\n",
    "        train_score.append(sum(c_train))\n",
    "        val_score.append(sum(c_val))\n",
    "        x_axis.append(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print \"Final Train cost: {}, on Epoch {}\".format(train_score[-1],k)\n",
    "print \"Final Validation cost: {}, on Epoch {}\".format(val_score[-1],k)\n",
    "plt.plot(train_score, 'r-', val_score, 'b-')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "##This part generates a new validation set to test against\n",
    "val_score_v =[]\n",
    "num_epochs=1\n",
    "\n",
    "for k in range(num_epochs):\n",
    "\n",
    "    #Generate Data for each epoch\n",
    "    tempX,y = gen_data(5,seq_len,batch_size)\n",
    "    X = []\n",
    "    for i in range(seq_len):\n",
    "        X.append(tempX[:,i,:])\n",
    "\n",
    "    val_dict = {inputs[i]:X[i] for i in range(seq_len)}\n",
    "    val_dict.update({result: y})\n",
    "    outv, c_val = sess.run([outputs3,cost],feed_dict = val_dict ) \n",
    "    val_score_v.append([c_val])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "##Target\n",
    "tempX[3],y[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Prediction\n",
    "outv[3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example\n",
    "\n",
    "A Recurrent Neural Network (LSTM) implementation example using TensorFlow library.\n",
    "\n",
    "This example is using the MNIST database of handwritten digits (http://yann.lecun.com/exdb/mnist/)\n",
    "Long Short Term Memory paper: http://deeplearning.cs.cmu.edu/pdfs/Hochreiter97_lstm.pdf\n",
    "\n",
    "Author: Aymeric Damien\n",
    "Project: https://github.com/aymericdamien/TensorFlow-Examples/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.python.ops import rnn, rnn_cell\n",
    "import numpy as np\n",
    "tf.reset_default_graph()\n",
    "\n",
    "# Import MINST data\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"data/\", one_hot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To classify images using a recurrent neural network, we consider every image row as a sequence of pixels. \n",
    "\n",
    "Because MNIST image shape is $28 \\times 28$ px, we will then handle 28 sequences of 28 steps for every sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Parameters\n",
    "learning_rate  = 0.001\n",
    "training_iters = 100000\n",
    "batch_size     = 128\n",
    "\n",
    "display_step   = 50\n",
    "\n",
    "# Network Parameters\n",
    "n_input        = 28 # number of sequences for every sample\n",
    "n_steps        = 28 # number of timesteps for every sequence\n",
    "n_hidden       = 64 # hidden layer num of features\n",
    "n_classes      = 10 # total classes (0-9 digits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# tf Graph input\n",
    "x = tf.placeholder(\"float\", [None, n_steps, n_input])\n",
    "\n",
    "\n",
    "y = tf.placeholder(\"float\", [None, n_classes])\n",
    "\n",
    "# Define weights\n",
    "weights = {\n",
    "    'hidden': tf.Variable(tf.random_normal([n_input, n_hidden])), \n",
    "    # Hidden layer weights\n",
    "    'out': tf.Variable(tf.random_normal([n_hidden, n_classes]))\n",
    "}\n",
    "biases = {\n",
    "    'hidden': tf.Variable(tf.random_normal([n_hidden])),\n",
    "    'out': tf.Variable(tf.random_normal([n_classes]))\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def RNN(x, weights, biases):\n",
    "\n",
    "    # Prepare data shape to match `rnn` function requirements\n",
    "    # Current data input shape: (batch_size, n_steps, n_input)\n",
    "    # Required shape: 'n_steps' tensors list of shape (batch_size, n_input)\n",
    "    \n",
    "    # Permuting batch_size and n_steps\n",
    "    x = tf.transpose(x, [1, 0, 2])\n",
    "    # Reshaping to (n_steps*batch_size, n_input)\n",
    "    x = tf.reshape(x, [-1, n_input])\n",
    "    # Split to get a list of 'n_steps' tensors of shape (batch_size, n_input)\n",
    "    x = tf.split(0, n_steps, x)\n",
    "\n",
    "    # Define a lstm cell with tensorflow\n",
    "    lstm_cell = rnn_cell.BasicLSTMCell(n_hidden, forget_bias=1.0)\n",
    "\n",
    "    # Get lstm cell output\n",
    "    outputs, states = rnn.rnn(lstm_cell, x, dtype=tf.float32)\n",
    "\n",
    "    # Linear activation, using rnn inner loop last output\n",
    "    return tf.matmul(outputs[-1], weights['out']) + biases['out']\n",
    "\n",
    "pred = RNN(x, weights, biases)\n",
    "\n",
    "# Define loss and optimizer\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(pred, y))\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "\n",
    "# Evaluate model\n",
    "correct_pred = tf.equal(tf.argmax(pred,1), tf.argmax(y,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "\n",
    "# Initializing the variables\n",
    "init = tf.initialize_all_variables()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Launch the graph\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    step = 1\n",
    "    # Keep training until reach max iterations\n",
    "    while step * batch_size < training_iters:\n",
    "        batch_x, batch_y = mnist.train.next_batch(batch_size)\n",
    "        # Reshape data to get 28 seq of 28 elements\n",
    "        batch_x = batch_x.reshape((batch_size, n_steps, n_input))\n",
    "        # Run optimization op (backprop)\n",
    "        sess.run(optimizer, feed_dict={x: batch_x, y: batch_y})\n",
    "        if step % display_step == 0:\n",
    "            # Calculate batch accuracy\n",
    "            acc = sess.run(accuracy, feed_dict={x: batch_x, y: batch_y})\n",
    "            # Calculate batch loss\n",
    "            loss = sess.run(cost, feed_dict={x: batch_x, y: batch_y})\n",
    "            print \"Iter \" + str(step*batch_size) + \", Minibatch Loss= \" + \\\n",
    "                  \"{:.6f}\".format(loss) + \", Training Accuracy= \" + \\\n",
    "                  \"{:.5f}\".format(acc)\n",
    "        step += 1\n",
    "    print \"Optimization Finished!\"\n",
    "\n",
    "    # Calculate accuracy for 128 mnist test images\n",
    "    test_len = 128\n",
    "    test_data = mnist.test.images[:test_len].reshape((-1, n_steps, n_input))\n",
    "    test_label = mnist.test.labels[:test_len]\n",
    "    print \"Testing Accuracy:\", \\\n",
    "        sess.run(accuracy, feed_dict={x: test_data, y: test_label})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bidirectional LSTM\n",
    "\n",
    "So  far,  we  have  focused  on  RNNs  that  look  into  the  past    to predict future values in the sequence, but not to  to make predictions based on future values by reading throught the sequence backwards?\n",
    "\n",
    "**Bi-directional  deep  neural networs**,  at each time-step, $t$,  maintain two hidden layers, one for the left-to-right propagation and another for the right-to-left  propagation (hence, consuming twice  as  much  memory  space).   \n",
    "\n",
    "The final classification result, $\\hat{y}$, is generated through combining the score results produced by both RNN hidden layers.\n",
    "\n",
    "<img src = \"./images/t9.png\"  width = \"600\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.python.ops import rnn, rnn_cell\n",
    "import numpy as np\n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "# Import MINST data\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"data/\", one_hot=True)\n",
    "\n",
    "# Parameters\n",
    "learning_rate = 0.001\n",
    "training_iters = 100000\n",
    "batch_size = 128\n",
    "display_step = 100\n",
    "\n",
    "# Network Parameters\n",
    "n_input = 28 # MNIST data input (img shape: 28*28)\n",
    "n_steps = 28 # timesteps\n",
    "n_hidden = 128 # hidden layer num of features\n",
    "n_classes = 10 # MNIST total classes (0-9 digits)\n",
    "\n",
    "# tf Graph input\n",
    "x = tf.placeholder(\"float\", [None, n_steps, n_input])\n",
    "y = tf.placeholder(\"float\", [None, n_classes])\n",
    "\n",
    "# Define weights\n",
    "weights = {\n",
    "    # Hidden layer weights => 2*n_hidden because of foward + backward cells\n",
    "    'out': tf.Variable(tf.random_normal([2*n_hidden, n_classes]))\n",
    "}\n",
    "biases = {\n",
    "    'out': tf.Variable(tf.random_normal([n_classes]))\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def BiRNN(x, weights, biases):\n",
    "\n",
    "    # Prepare data shape to match `bidirectional_rnn` function requirements\n",
    "    # Current data input shape: (batch_size, n_steps, n_input)\n",
    "    # Required shape: 'n_steps' tensors list of shape (batch_size, n_input)\n",
    "    \n",
    "    # Permuting batch_size and n_steps\n",
    "    x = tf.transpose(x, [1, 0, 2])\n",
    "    # Reshape to (n_steps*batch_size, n_input)\n",
    "    x = tf.reshape(x, [-1, n_input])\n",
    "    # Split to get a list of 'n_steps' tensors of shape (batch_size, n_input)\n",
    "    x = tf.split(0, n_steps, x)\n",
    "\n",
    "    # Define lstm cells with tensorflow\n",
    "    # Forward direction cell\n",
    "    lstm_fw_cell = rnn_cell.BasicLSTMCell(n_hidden, forget_bias=1.0)\n",
    "    # Backward direction cell\n",
    "    lstm_bw_cell = rnn_cell.BasicLSTMCell(n_hidden, forget_bias=1.0)\n",
    "\n",
    "    # Get lstm cell output\n",
    "    outputs, _, _ = rnn.bidirectional_rnn(lstm_fw_cell, \n",
    "                                          lstm_bw_cell, \n",
    "                                          x,\n",
    "                                          dtype=tf.float32)\n",
    "    \n",
    "    # Linear activation, using rnn inner loop last output\n",
    "    return tf.matmul(outputs[-1], weights['out']) + biases['out']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pred = BiRNN(x, weights, biases)\n",
    "\n",
    "# Define loss and optimizer\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(pred, y))\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "\n",
    "# Evaluate model\n",
    "correct_pred = tf.equal(tf.argmax(pred,1), tf.argmax(y,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "\n",
    "# Initializing the variables\n",
    "init = tf.initialize_all_variables()\n",
    "\n",
    "# Launch the graph\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    step = 1\n",
    "    # Keep training until reach max iterations\n",
    "    while step * batch_size < training_iters:\n",
    "        batch_x, batch_y = mnist.train.next_batch(batch_size)\n",
    "        # Reshape data to get 28 seq of 28 elements\n",
    "        batch_x = batch_x.reshape((batch_size, n_steps, n_input))\n",
    "        # Run optimization op (backprop)\n",
    "        sess.run(optimizer, feed_dict={x: batch_x, y: batch_y})\n",
    "        if step % display_step == 0:\n",
    "            # Calculate batch accuracy\n",
    "            acc = sess.run(accuracy, feed_dict={x: batch_x, y: batch_y})\n",
    "            # Calculate batch loss\n",
    "            loss = sess.run(cost, feed_dict={x: batch_x, y: batch_y})\n",
    "            print \"Iter \" + str(step*batch_size) + \", Minibatch Loss= \" + \\\n",
    "                  \"{:.6f}\".format(loss) + \", Training Accuracy= \" + \\\n",
    "                  \"{:.5f}\".format(acc)\n",
    "        step += 1\n",
    "    print \"Optimization Finished!\"\n",
    "\n",
    "    # Calculate accuracy for 128 mnist test images\n",
    "    test_len = 128\n",
    "    test_data = mnist.test.images[:test_len].reshape((-1, n_steps, n_input))\n",
    "    test_label = mnist.test.labels[:test_len]\n",
    "    print \"Testing Accuracy:\", \\\n",
    "        sess.run(accuracy, feed_dict={x: test_data, y: test_label})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Name generation with LSTM\n",
    "\n",
    "We are going to train RNN \"character-level\" language models. \n",
    "\n",
    "That is, we’ll give the RNN a huge chunk of text and ask it to model the probability distribution of the next character in the sequence given a sequence of previous characters. This will then allow us to generate new text one character at a time.\n",
    "\n",
    "We will encode each character into a vector using ``1-of-k`` encoding (i.e. all zero except for a single one at the index of the character in the vocabulary), and feed them into the RNN one at a time. \n",
    "\n",
    "At test time, we will feed a character into the RNN and get a distribution over what characters are likely to come next. We sample from this distribution, and feed it right back in to get the next letter. Repeat this process and you’re sampling text!\n",
    "\n",
    "We can also play with the temperature of the Softmax during sampling. Decreasing the temperature from 1 to some lower number (e.g. 0.5) makes the RNN more confident, but also more conservative in its samples. Conversely, higher temperatures will give more diversity but at cost of more mistakes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to process sequences of symbols with RNN we need to represent these symbols by numbers.\n",
    "\n",
    "Let's suppose we have $|V|$ different symbols. The most simple representation is the **one-hot vector**: Represent every word as an $\\mathbb{R}^{|V|\\times1}$ vector with all $0$s and one $1$ at the index of that word in the sorted english language. Symbol vectors in this type of encoding would appear as the following:\n",
    "\n",
    "$$w^{s_1} = \\left[ \\begin{array}{c} 1 \\\\ 0 \\\\ 0 \\\\ \\vdots \\\\ 0 \\end{array} \\right], w^{s_2} = \\left[ \\begin{array}{c} 0 \\\\ 1 \\\\ 0 \\\\ \\vdots \\\\ 0 \\end{array} \\right], w^{s_3} = \\left[ \\begin{array}{c} 0 \\\\ 0 \\\\ 1 \\\\ \\vdots \\\\ 0 \\end{array} \\right], \\cdots \n",
    "w^{s_{|V|}} = \\left[ \\begin{array}{c} 0 \\\\ 0 \\\\ 0 \\\\ \\vdots \\\\ 1 \\end{array} \\right] $$\n",
    "\n",
    "We represent each symbol as a completely independent entity. This symbol representation does not give us directly any notion of similarity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To train our model we need text to learn from a large dataset of names. Fortunately we don’t need any labels to train a language model, just raw text.\n",
    "\n",
    "Places names: You can download 52,700 Catalan names from a dataset available on http://territori.gencat.cat/ca/01_departament/11_normativa_i_documentacio/03_documentacio/02_territori_i_mobilitat/cartografia/nomenclator_oficial_de_toponimia_de_catalunya/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation\n",
    "from keras.layers import LSTM\n",
    "from keras.optimizers import RMSprop\n",
    "import numpy as np\n",
    "import random\n",
    "import sys\n",
    "\n",
    "import codecs\n",
    "f = codecs.open('data/womennamesbarcelona.txt', \"r\", \"utf-8\")\n",
    "#f = codecs.open('data/toponims.txt', \"r\", \"utf-8\")\n",
    "string = f.read()\n",
    "string.encode('utf-8')\n",
    "text = string.lower()\n",
    "\n",
    "# text = text.replace(\"\\n\", \" \")\n",
    "    \n",
    "print('corpus length:', len(text))\n",
    "\n",
    "chars = sorted(list(set(text)))\n",
    "print('total chars:', len(chars))\n",
    "char_indices = dict((c, i) for i, c in enumerate(chars))\n",
    "indices_char = dict((i, c) for i, c in enumerate(chars))\n",
    "\n",
    "# cut the text in semi-redundant sequences of maxlen characters\n",
    "maxlen = 20\n",
    "step = 3\n",
    "sentences = []\n",
    "next_chars = []\n",
    "for i in range(0, len(text) - maxlen, step):\n",
    "    sentences.append(text[i: i + maxlen])\n",
    "    next_chars.append(text[i + maxlen])\n",
    "print('nb sequences:', len(sentences))\n",
    "\n",
    "print('Vectorization...')\n",
    "X = np.zeros((len(sentences), maxlen, len(chars)), dtype=np.bool)\n",
    "y = np.zeros((len(sentences), len(chars)), dtype=np.bool)\n",
    "for i, sentence in enumerate(sentences):\n",
    "    for t, char in enumerate(sentence):\n",
    "        X[i, t, char_indices[char]] = 1\n",
    "    y[i, char_indices[next_chars[i]]] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# build the model\n",
    "\n",
    "print('Build model...')\n",
    "model = Sequential()\n",
    "model.add(LSTM(64, \n",
    "#               return_sequences=True,\n",
    "               dropout_W=0.2, \n",
    "               dropout_U=0.2, \n",
    "               input_shape=(maxlen, len(chars))))\n",
    "#model.add(LSTM(64, \n",
    "#               dropout_W=0.2, \n",
    "#               dropout_U=0.2))\n",
    "model.add(Dense(len(chars)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "optimizer = RMSprop(lr=0.01)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=optimizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The simplest way to use the Keras LSTM model to make predictions is to first start off with a seed sequence as input, generate the next character then update the seed sequence to add the generated character on the end and trim off the first character. \n",
    "\n",
    "This process is repeated for as long as we want to predict new characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def sample(preds, temperature=1.0):\n",
    "    # helper function to sample an index from a probability array\n",
    "    preds = np.asarray(preds).astype('float64')\n",
    "    preds = np.log(preds) / temperature\n",
    "    exp_preds = np.exp(preds)\n",
    "    preds = exp_preds / np.sum(exp_preds)\n",
    "    probas = np.random.multinomial(1, preds, 1)\n",
    "    return np.argmax(probas)\n",
    "\n",
    "# train the model, output generated text after each iteration\n",
    "for iteration in range(1, 60):\n",
    "    print()\n",
    "    print('-' * 50)\n",
    "    print('Iteration', iteration)\n",
    "    model.fit(X, y, batch_size=256, nb_epoch=1)\n",
    "\n",
    "    start_index = random.randint(0, len(text) - maxlen - 1)\n",
    "    generated = ''\n",
    "    sentence = text[start_index: start_index + maxlen]\n",
    "    generated += sentence\n",
    "    print('----- Generating with seed: \"' + sentence.replace(\"\\n\", \" \") + '\"')\n",
    "        \n",
    "    for diversity in [0.5, 1.0, 1.2]:\n",
    "        print()\n",
    "        print('----- diversity:', diversity)\n",
    "        for i in range(50):\n",
    "            \n",
    "            x = np.zeros((1, maxlen, len(chars)))\n",
    "            for t, char in enumerate(sentence):\n",
    "                x[0, t, char_indices[char]] = 1.\n",
    "\n",
    "            preds = model.predict(x, verbose=0)[0]\n",
    "            next_index = sample(preds, diversity)\n",
    "            next_char = indices_char[next_index]\n",
    "\n",
    "            generated += next_char\n",
    "            sentence = sentence[1:] + next_char\n",
    "\n",
    "            sys.stdout.write(next_char)\n",
    "            sys.stdout.flush()\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "'lianda' in text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "In the case of places, after several hours you can generate names such as:\n",
    "\n",
    "+ Alzinetes, torrent de les\n",
    "+ Alzinetes, vall de les\n",
    "+ **Alzinó, Mas d'**\n",
    "+ Alzinosa, collada de l'\n",
    "+ Alzinosa, font de l'\n",
    "\n",
    "-\n",
    "\n",
    "+ Benavent, roc de\n",
    "+ Benaviure, Cal\n",
    "+ **Benca**\n",
    "+ Bendiners, pla de\n",
    "+ Benedi, roc del\n",
    "\n",
    "-\n",
    "\n",
    "+ Fiola, la\n",
    "+ Fiola, puig de la\n",
    "+ **Fiper, Granja del**\n",
    "+ Firassa, Finca\n",
    "+ Firell\n",
    "\n",
    "-\n",
    "\n",
    "+ Regueret, lo\n",
    "+ Regueret, lo\n",
    "+ **Regueró**\n",
    "+ Reguerols, els\n",
    "+ Reguerons, els\n",
    "\n",
    "-\n",
    "\n",
    "+ Vallverdú, Mas de\n",
    "+ Vallverdú, serrat de\n",
    "+ **Vallvicamanyà**\n",
    "+ Vallvidrera\n",
    "+ Vallvidrera, riera de\n",
    "\n",
    "-\n",
    "\n",
    "+ Terraubella, Corral de\n",
    "+ Terraubes\n",
    "+ **Terravanca**\n",
    "+ Terrer Nou, Can\n",
    "+ Terrer Roig, lo\n",
    "\n",
    "where names in **bold** are generated and other names are the nearest neighbours (in the training dataset) of the generated name."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
